{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install pymupdf\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import pdfplumber\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "from googletrans import Translator\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLUslZWvKQ0h",
        "outputId": "aebc16fc-e062-4210-e189-d8afc55afe44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.10.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf:\n",
        "        for page in pdf:\n",
        "            page_text = page.get_text()\n",
        "            clean_text = page_text\n",
        "            text += clean_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def split_text(text, max_length=1000):\n",
        "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "def generate_question(sentence):\n",
        "    location_rule = re.search(r'(.+?) (में|से|पर) (.+?) है।', sentence)\n",
        "    if location_rule:\n",
        "        location = location_rule.group(1)\n",
        "        location_without_last_word = ' '.join(location.split()[:-1])\n",
        "        return f\"{location_without_last_word} कहाँ {location_rule.group(3)} है?\"\n",
        "\n",
        "    who_rule = re.search(r'(.+?) ने (.+?)।', sentence)\n",
        "    if who_rule:\n",
        "        return f\"किसने {who_rule.group(2)}?\"\n",
        "\n",
        "    ki_rule = re.search(r'(.+?) की (.+?)।', sentence)\n",
        "    if ki_rule:\n",
        "        location = ki_rule.group(1)\n",
        "        location_without_last_word = ' '.join(location.split()[:-1])\n",
        "        return f\"{location_without_last_word} किसकी {ki_rule.group(2)}?\"\n",
        "\n",
        "    return \"Question generation rule not found.\"\n",
        "\n",
        "def process_book(sentences):\n",
        "    data = []\n",
        "\n",
        "    for sentence in tqdm(sentences, desc=\"Generating Questions\"):\n",
        "        question = generate_question(sentence)\n",
        "        if question:\n",
        "            data.append({'Sentence': sentence, 'Question': question})\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['Sentence', 'Question'])\n",
        "\n",
        "    return df\n",
        "\n",
        "path = input(\"Enter the path of the PDF file: \")\n",
        "text = extract_text_from_pdf(path)\n",
        "sentences = re.split(r'(?<=।)\\s+', text)\n",
        "\n",
        "df = process_book(sentences)\n",
        "\n",
        "answers = df[df.Question != \"Question generation rule not found.\"]\n",
        "\n",
        "\n",
        "# Save the filtered results to a TXT file\n",
        "output_file_txt = \"generated_questions.txt\"\n",
        "with open(output_file_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "    for index, row in answers.iterrows():\n",
        "        f.write(f\"Sentence: {row['Sentence']}\\n\")\n",
        "        f.write(f\"Question: {row['Question']}\\n\\n\")\n",
        "\n",
        "print(f\"Questions and sentences saved to '{output_file_txt}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTxFXE-rKdr4",
        "outputId": "760bf5ba-648d-4a5e-c6d9-1bbdd8c1b145"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path of the PDF file: /content/godan.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Questions: 100%|██████████| 10378/10378 [00:02<00:00, 4837.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions and sentences saved to 'generated_questions.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7GPScwcKBETb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8MuDeNMSXQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model 2\n",
        "translator = Translator()\n",
        "\n",
        "def extract_clean_text_chunks_from_pdf(pdf_path, chunk_size=1000):\n",
        "    text_chunks = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                page_text = re.sub(r'http\\S+|www\\S+|file:\\S+|\\S+\\.html', '', page_text)\n",
        "                page_text = re.sub(r'\\s+', ' ', page_text).strip()\n",
        "                for i in range(0, len(page_text), chunk_size):\n",
        "                    chunk = page_text[i:i + chunk_size]\n",
        "                    if chunk:\n",
        "                        text_chunks.append(chunk)\n",
        "    return text_chunks\n",
        "\n",
        "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qa-qg-hl\")\n",
        "\n",
        "def generate_questions(text, num_questions=5):\n",
        "    if not text:\n",
        "        return []\n",
        "    formatted_text = \"generate question: \" + text\n",
        "    questions = question_generator(formatted_text, max_length=100, num_beams=5, num_return_sequences=num_questions)\n",
        "    return questions\n",
        "\n",
        "def retrieve_relevant_chunks(prompt, text_chunks, top_n=5):\n",
        "    if not text_chunks:\n",
        "        return []\n",
        "    vectorizer = TfidfVectorizer().fit_transform([prompt] + text_chunks)\n",
        "    cosine_similarities = cosine_similarity(vectorizer[0:1], vectorizer[1:]).flatten()\n",
        "    top_n_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "    relevant_chunks = [text_chunks[i] for i in top_n_indices]\n",
        "    return relevant_chunks\n",
        "\n",
        "def safe_translate(text, src, dest):\n",
        "    try:\n",
        "        if text is None or text.strip() == \"\":\n",
        "            print(f\"No text provided for translation from {src} to {dest}.\")\n",
        "            return \"\"\n",
        "        translated_text = translator.translate(text, src=src, dest=dest).text\n",
        "        return translated_text\n",
        "    except Exception as e:\n",
        "        return \"\"\n",
        "\n",
        "def generate_questions_from_prompt_with_rag(prompt, pdf_text, total_questions=10, top_n_chunks=5, questions_per_chunk=3):\n",
        "    translated_prompt = safe_translate(prompt, src='hi', dest='en')\n",
        "\n",
        "    if not translated_prompt:\n",
        "        print(\"Prompt translation failed. Exiting.\")\n",
        "        return []\n",
        "\n",
        "    relevant_chunks = retrieve_relevant_chunks(translated_prompt, pdf_text, top_n=top_n_chunks)\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        print(\"No relevant chunks found.\")\n",
        "        return []\n",
        "\n",
        "    all_generated_questions = []\n",
        "    for chunk in relevant_chunks:\n",
        "        translated_chunk = safe_translate(chunk, src='hi', dest='en')\n",
        "        if not translated_chunk:\n",
        "            print(\"Chunk translation failed, skipping this chunk.\")\n",
        "            continue\n",
        "\n",
        "        questions = generate_questions(translated_chunk, num_questions=questions_per_chunk)\n",
        "\n",
        "        for q in questions:\n",
        "            cleaned_question = re.sub(r'\\s+', ' ', q['generated_text']).strip()\n",
        "            if cleaned_question:\n",
        "                all_generated_questions.append(cleaned_question)\n",
        "\n",
        "        if len(all_generated_questions) >= total_questions:\n",
        "            break\n",
        "\n",
        "    translated_questions = []\n",
        "    for q in all_generated_questions:\n",
        "        translated_q = safe_translate(q, src='en', dest='hi')\n",
        "        if translated_q:\n",
        "            translated_questions.append(translated_q)\n",
        "\n",
        "    return translated_questions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = input(\"Enter the path of the PDF file: \")\n",
        "    prompt = input(\"Enter a prompt (in Hindi): \")\n",
        "\n",
        "    text_chunks = extract_clean_text_chunks_from_pdf(pdf_path)\n",
        "\n",
        "    if text_chunks:\n",
        "        print(\"Text chunks extracted from the PDF.\")\n",
        "    else:\n",
        "        print(\"No text found in the PDF.\")\n",
        "\n",
        "    if text_chunks:\n",
        "        print(f\"Generating questions based on the prompt: '{prompt}'...\")\n",
        "        generated_questions = generate_questions_from_prompt_with_rag(\n",
        "            prompt, text_chunks, total_questions=10, top_n_chunks=5, questions_per_chunk=2\n",
        "        )\n",
        "\n",
        "        if generated_questions:\n",
        "            print(\"Generated Questions:\")\n",
        "            for idx, question in enumerate(generated_questions, 1):\n",
        "                print(f\"{idx}. {question}\")\n",
        "        else:\n",
        "            print(\"No questions were generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNUYbPKsKrjQ",
        "outputId": "41fa3b95-f030-401d-aed7-6c0a0a399e74"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path of the PDF file: /content/godan.pdf\n",
            "Enter a prompt (in Hindi): होरी कंधों पर लाठी रख कर घर से ननकला,\n",
            "Text chunks extracted from the PDF.\n",
            "Generating questions based on the prompt: 'होरी कंधों पर लाठी रख कर घर से ननकला,'...\n",
            "Generated Questions:\n",
            "1. सतू ली की संरचना का क्या हुआ?\n",
            "2. धान्याय यंत की सतू ली संरचना का क्या हुआ?\n",
            "3. धन्य्यासभा ने गले से क्या कहा और कहा - पंचो, आपको गरीबों को सताते हुए खुशी नहीं मिलेगी?\n",
            "4. धन्य्यासभा ने गले से क्या कहा और कहा - पंचो, आपको गरीबों को सताते हुए खुशी नहीं मिलेगी?\n",
            "5. झुन्नाया का सबसे गहरा और नीच 214 क्या था?\n",
            "6. झुन्नाया की नीली 214 क्या थी?\n",
            "7. अम्मुन झुन्नाया का क्या हुआ?\n",
            "8. जब वह नासर पर कराह रहा था तो अम्मुन झुन्नाया का क्या हुआ?\n",
            "9. झुन्नाया ने एक मन से क्या कहा - आपका अम्मुन बहुत गुस्से में है?\n",
            "10. झुन्नाया ने एक मन से क्या कहा?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y0eB-A_qShdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}